{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13048941,"sourceType":"datasetVersion","datasetId":8263090}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n# âœ… Import RandomErasing from the standard torchvision library\nfrom torchvision.transforms import RandomErasing\nfrom torch.cuda.amp import autocast, GradScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport timm\nfrom torch import amp\nimport numpy as np\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# Seeding for Reproducibility\n# =============================\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set your desired seed\nSEED = 42\nseed_everything(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# Configuration (Updated for AdamW)\n# =============================\ndata_dir = \"/kaggle/input/sports-102/Sports102_V2\"\noutput_dir = \"/kaggle/working/swinv2_sydnet_adamw_outputs\"\nos.makedirs(output_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- OPTIMIZED SETTINGS FOR TRANSFORMER TRAINING ---\nbatch_size = 32      # Increased for stability and speed\nnum_epochs = 50      # Set to 50 as requested\nlearning_rate = 2e-4   # A robust learning rate for AdamW\n# ---------------------------------------------------\n\nlog_interval = 10  # Log loss every 10 mini-batches\nimg_size = 224","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# Data Loading and Transforms\n# =============================\n\n# --- ADDED AUGMENTATIONS AS PER THE PAPER ---\n# To achieve two-region erasing with older torchvision versions,\n# we apply the single-region RandomErasing transform twice.\ntrain_transform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.RandomHorizontalFlip(),\n    # Paper specifies random rotation and scaling \n    transforms.RandomRotation(25), \n    transforms.RandomAffine(0, scale=(0.75, 1.25)), # Scale of 1 +/- 0.25\n    # ----------------------------------------\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n    # Add the SYD-Net random erasing technique manually\n    transforms.RandomErasing(p=0.5, value='random'), # First erased region\n    transforms.RandomErasing(p=0.5, value='random'), # Second erased region\n])\n# ---------------------------------------------\n\ntest_transform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\n# This split is now reproducible thanks to torch.manual_seed()\nfull_train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transform)\ntrain_size = int(0.8 * len(full_train_dataset))\nval_size = len(full_train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\ntest_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=test_transform)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(SEED)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=4,\n    worker_init_fn=seed_worker,\n    generator=g\n)\n\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\nnum_classes = len(full_train_dataset.classes)\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# Model Setup (Updated for AdamW)\n# =============================\n\n# 1. Implement Gaussian Dropout as described in the SYD-Net paper\nclass GaussianDropout(nn.Module):\n    def __init__(self, p=0.1):\n        super(GaussianDropout, self).__init__()\n        if p < 0 or p > 1:\n            raise ValueError(\"dropout probability has to be between 0 and 1, but got {}\".format(p))\n        self.p = p\n        self.sigma = (p / (1.0 - p)) ** 0.5 if p > 0 else 0\n\n    def forward(self, x):\n        if self.training and self.p > 0:\n            noise = torch.randn_like(x) * self.sigma\n            return x * (1 + noise)\n        return x\n\n# 2. Implement the EXACT Patch-based Attention (PbA) Module from the paper\nclass ExactSpatialAttention(nn.Module):\n    \"\"\" Implements the Spatial Attention (SA) path exactly as described. \"\"\"\n    def __init__(self, in_features):\n        super().__init__()\n        self.in_features = in_features\n        self.mlp = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features * 2, in_features),\n            nn.Softmax(dim=1),\n            GaussianDropout(0.2),\n            nn.BatchNorm1d(in_features)\n        )\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.gmp = nn.AdaptiveMaxPool2d(1)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        gap_feat = self.gap(x).view(B, C)\n        gmp_feat = self.gmp(x).view(B, C)\n        fused_feat = torch.cat([gap_feat, gmp_feat], dim=1)\n        sa_mask = self.mlp(fused_feat)\n        return sa_mask\n\nclass ExactChannelAttention(nn.Module):\n    \"\"\" Implements the complex pairwise Channel Attention (CA) path. \"\"\"\n    def __init__(self, in_features):\n        super().__init__()\n        self.W_psi = nn.Linear(in_features, in_features, bias=False)\n        self.W_psi_prime = nn.Linear(in_features, in_features, bias=True)\n        self.W_theta = nn.Linear(in_features, in_features, bias=True)\n        self.W_delta = nn.Linear(in_features, 1, bias=True)\n        self.W_phi = nn.Linear(in_features, in_features, bias=True)\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, patches):\n        B, C = patches[0].size(0), patches[0].size(1)\n        patch_vectors = [self.gap(p).view(B, 1, C) for p in patches]\n        patch_vectors = torch.cat(patch_vectors, dim=1)\n        q = self.W_psi(patch_vectors).unsqueeze(2)\n        k = self.W_psi_prime(patch_vectors).unsqueeze(1)\n        psi = torch.tanh(q + k)\n        vartheta = torch.sigmoid(self.W_theta(psi))\n        delta = torch.softmax(self.W_delta(vartheta), dim=2)\n        f_hat = torch.sum(delta * patch_vectors.unsqueeze(1), dim=2)\n        phi = torch.softmax(self.W_phi(f_hat), dim=1)\n        f_ca = torch.sum(phi * f_hat, dim=1)\n        return f_ca\n\nclass ExactPbA(nn.Module):\n    \"\"\" The main PbA module combining SA and CA. \"\"\"\n    def __init__(self, in_features, patch_grid_size=2):\n        super().__init__()\n        self.sa = ExactSpatialAttention(in_features)\n        self.ca = ExactChannelAttention(in_features)\n        self.patch_grid_size = patch_grid_size\n\n    def forward(self, x):\n        f_sa = self.sa(x)\n        B, C, H, W = x.shape\n        patch_h, patch_w = H // self.patch_grid_size, W // self.patch_grid_size\n        patches = [x[..., i*patch_h:(i+1)*patch_h, j*patch_w:(j+1)*patch_w] for i in range(self.patch_grid_size) for j in range(self.patch_grid_size)]\n        f_ca = self.ca(patches)\n        f_pba = f_ca * f_sa + f_ca\n        return f_pba\n\n# 3. Create the final model combining the SwinV2 Transformer with the exact SYD-Net PbA\nclass SwinWithSYDNetTechniques(nn.Module):\n    def __init__(self, num_classes=102, pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"swinv2_cr_small_ns_224\",\n            pretrained=True,\n            features_only=True,\n            out_indices=(-1,)\n        )\n        self.in_features = self.backbone.feature_info.channels()[-1]\n        self.pba = ExactPbA(self.in_features)\n        # âœ… CORRECTED this block by adding the closing parenthesis\n        self.classification_head = nn.Sequential(\n            GaussianDropout(0.2),\n            nn.BatchNorm1d(self.in_features),\n            nn.Linear(self.in_features, num_classes)\n        )\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, x):\n        feature_map = self.backbone(x)[-1]\n        f_pba = self.pba(feature_map)\n        f_gap = self.gap(feature_map).view(-1, self.in_features)\n        final_features = f_pba + f_gap\n        output = self.classification_head(final_features)\n        return output\n\n# Instantiate the NEW model\nmodel = SwinWithSYDNetTechniques(num_classes=num_classes, pretrained=True)\nmodel = model.to(device)\n\n# --- SETUP OPTIMIZER & SCHEDULER FOR TRANSFORMER TRAINING ---\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\nscaler = torch.amp.GradScaler(device='cuda')\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=3, factor=0.5)\n# ----------------------------------------------------\nprint(f\"Using SwinV2 backbone with AdamW optimizer. Initial learning rate: {optimizer.param_groups[0]['lr']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# Resume Checkpoint\n# =============================\nstart_epoch = 0\nbest_val_acc = 0.0\ncheckpoint_path = os.path.join(output_dir, \"checkpoint.pth\")\nbest_model_path = os.path.join(output_dir, \"best_model.pth\")\n\nif os.path.exists(checkpoint_path):\n    print(\"Resuming from checkpoint...\")\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n    best_val_acc = checkpoint[\"best_val_acc\"]\n    start_epoch = checkpoint[\"epoch\"] + 1\n    print(f\"Resumed from epoch {start_epoch} with best val acc {best_val_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# Training Loop\n# =============================\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(start_epoch, num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct, total = 0, 0\n    \n    for i, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        with amp.autocast(device_type=\"cuda\"):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n\n    train_acc = 100 * correct / total\n    train_loss = running_loss / len(train_loader)\n    train_losses.append(train_loss)\n\n    # Validation\n    model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    with torch.no_grad(), amp.autocast(device_type=\"cuda\"):\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            val_total += labels.size(0)\n            val_correct += predicted.eq(labels).sum().item()\n\n    val_acc = 100 * val_correct / val_total\n    val_loss /= len(val_loader)\n    val_losses.append(val_loss)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n          f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n\n    # --- UPDATED SCHEDULER STEP ---\n    scheduler.step(val_acc)\n\n    # Checkpointing (every epoch)\n    torch.save({\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"scheduler_state_dict\": scheduler.state_dict(),\n        \"best_val_acc\": best_val_acc,\n    }, checkpoint_path)\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(\"âœ… Saved best model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# Evaluation (Train, Val, Test)\n# =============================\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndef evaluate(loader, name):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n        for images, labels in tqdm(loader, desc=f\"Evaluating {name}\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = outputs.max(1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    report = classification_report(all_labels, all_preds, output_dict=True, target_names=full_train_dataset.classes)\n    print(f\"\\n{name} Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=full_train_dataset.classes))\n    acc = report[\"accuracy\"] * 100\n    return report, all_labels, all_preds\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, normalize=False, figsize=(30, 30), fontsize=6, save_path=None):\n    cm = confusion_matrix(y_true, y_pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.figure(figsize=figsize)\n    sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=\"Blues\",\n                xticklabels=class_names, yticklabels=class_names, cbar=True)\n\n    plt.ylabel('True label', fontsize=fontsize + 2)\n    plt.xlabel('Predicted label', fontsize=fontsize + 2)\n    plt.title('Confusion Matrix', fontsize=fontsize + 4)\n    plt.xticks(rotation=90, fontsize=fontsize)\n    plt.yticks(rotation=0, fontsize=fontsize)\n    plt.tight_layout()\n\n    if save_path:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"âœ… Confusion matrix saved to: {save_path}\")\n\n    plt.show()\n\nprint(\"\\nLoading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(best_model_path))\n\ntrain_report, _, _ = evaluate(train_loader, \"Train\")\nval_report, _, _ = evaluate(val_loader, \"Val\")\ntest_report, y_true, y_pred = evaluate(test_loader, \"Test\")\n\n# Print all accuracies together\nprint(\"\\nðŸ“Š Final Accuracies:\")\nprint(f\"Train Accuracy: {train_report['accuracy']*100:.2f}%\")\nprint(f\"Validation Accuracy: {val_report['accuracy']*100:.2f}%\")\nprint(f\"âœ… Test Accuracy: {test_report['accuracy']*100:.2f}%\")\n\n# Confusion Matrix for Test Set\nsave_path = os.path.join(output_dir, \"confusion_matrix.png\")\n\nprint(\"\\nðŸ”¹ Generating and saving confusion matrix for test set...\")\nplot_confusion_matrix(\n    y_true=y_true,\n    y_pred=y_pred,\n    class_names=full_train_dataset.classes,\n    normalize=False,  # Change to True if normalized matrix desired\n    figsize=(30, 30),\n    fontsize=6,\n    save_path=save_path\n)\n\nprint(\"âœ… Training complete. Confusion matrix saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}