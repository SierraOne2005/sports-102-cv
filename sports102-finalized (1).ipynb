{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13048941,"sourceType":"datasetVersion","datasetId":8263090},{"sourceId":13205617,"sourceType":"datasetVersion","datasetId":8369513}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## The Grand Strategy: One Model Per Week, in Multiple Sessions\nThe total training time is ~48 hours. Each of your two models will take ~24 hours. Since Kaggle sessions stop after 12 hours, we will train each model across three short sessions.\n\n1. Week 1: Train the convnext_large model (~24 hours total, split into three ~8-10 hour sessions).\n\n2. Week 2: Train the swinv2_large model (~24 hours total, split into three ~8-10 hour sessions), then run the final evaluation.\n\n## Week 1: Training the First Expert (convnext_large)\nYour goal this week is to use your 30-hour quota to fully train and save the convnext_large model.\n\n**Session 1.1 (Today)**\n\n1. Configure for Week 1: Go to Cell 2. Make sure the TRAIN_BACKBONES list contains only \"convnext_large.fb_in22k_ft_in1k\".\n2. Start the Run: Run Cells 1, 2, 3, and 4. They will execute quickly.\n3. Launch the Marathon: Run Cell 5.\n4. Monitor the Time: Keep an eye on the session timer at the top. Let it run for a good amount of time, for example, 8 to 10 hours.\n5. Stop and Save: Before the 12-hour limit, manually stop the session by clicking the Stop icon (■). Then, immediately commit your progress by clicking \"Save Version\" -> \"Save & Run All (Commit)\". This will save the crucial checkpoint file from this session.\n\n**Session 1.2 (T+1)**\n\n1. Start a New Session: Open the same notebook.\n2. CRITICAL - Add Previous Output: Click \"+ Add Input\" on the right. Go to the \"Notebook Output Files\" tab and find the run from Session 1.1. Add its output as an input source. This gives your new session access to the last checkpoint.\n3. Run the Cells: Run Cells 1 through 5 again. The script is smart. In Cell 4, it will automatically find the checkpoint from your attached input and resume training exactly where it left off.\n4. Repeat: Let it run for another 8-10 hours, then Stop and \"Save Version\" again.\n\n**Session 1.3 (T+2)**\n\n1. Repeat the process: Start a new session, add the output from Session 1.2 as an input, and run Cells 1-5.\n2. Completion: This final run should be short. The script will finish the remaining epochs for all three stages of the convnext_large model. Cell 5 will complete fully.\n3. Mission Complete for Week 1: The final, best model for convnext_large is now safely saved in your committed notebook's output. Do not run Cell 6.\n\n## Week 2: Training the Second Expert & Final Evaluation\nAfter your weekly quota resets (next weekend), you will repeat the process for the second model.\n\n**Session 2.1, 2.2, 2.3 (Next Weekend)**\n\n1. Configure for Week 2: Open the notebook. Go to Cell 2 and change the TRAIN_BACKBONES list to contain only \"swinv2_large_window12_384.ms_in22k_ft_in1k\".\n2. CRITICAL - Add Week 1's FINAL Model: Add the output from your final, completed Session 1.3 as an input. This makes the fully-trained convnext_large model available for the final evaluation later.\n3. Train the Second Model: Repeat the same multi-session process you used in Week 1. Run for 8-10 hours, stop, commit, start a new session, add the previous session's output, and run again.\n4. Completion: After about three sessions, Cell 5 will complete fully for the swinv2_large model.\n\n**Final Step: The Ultimate Result (Cell 6)**\n\n1. You are now in the session where Cell 5 has just finished for the second model.\n2. Scroll down to Cell 6.\n3. Run Cell 6.\n4. The Magic: The script will automatically:\n   1. Find the convnext_large model from the Week 1 output you attached.\n   2. Find the swinv2_large model from the current session's output.\n   3. Load both models, perform the TTA + Ensemble, and print your definitive, final accuracy score.","metadata":{}},{"cell_type":"markdown","source":"**Cell 1: Install Libraries**","metadata":{}},{"cell_type":"code","source":"!pip install timm scikit-learn --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:51:09.131074Z","iopub.execute_input":"2025-09-29T08:51:09.131572Z","iopub.status.idle":"2025-09-29T08:52:30.225688Z","shell.execute_reply.started":"2025-09-29T08:51:09.131547Z","shell.execute_reply":"2025-09-29T08:52:30.224999Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Cell 2: Imports and Configuration**","metadata":{}},{"cell_type":"code","source":"import os, math, time, random\nfrom collections import OrderedDict\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport timm\nfrom timm.data import Mixup\nfrom timm.loss import SoftTargetCrossEntropy\nfrom timm.utils import ModelEmaV2\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.swa_utils import AveragedModel, update_bn\n\n# --- CONFIGURATION ---\nDATA_PATH = \"/kaggle/input/sports-102/Sports102_V2\"\nSAVE_DIR = \"/kaggle/working/sydnet_full_stages\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# --- WEEK 1 CONFIGURATION ---\n# We are only training the first model this week.\nTRAIN_BACKBONES = [\n    \"convnext_large.fb_in22k_ft_in1k\",\n]\n\n# --- WEEK 2 CONFIGURATION ---\n# We are only training the second model this week.\n'''TRAIN_BACKBONES = [\n    \"swinv2_large_window12_384.ms_in22k_ft_in1k\",\n]'''\n\nSTAGE1_IMG, EPOCHS_STAGE1, PATIENCE_STAGE1 = 384, 120, 15 # Wait 15 epochs for improvement\nSTAGE2_IMG, EPOCHS_STAGE2, PATIENCE_STAGE2 = 512, 40, 8   # Wait 8 epochs for improvement\nSTAGE3_IMG, EPOCHS_STAGE3, PATIENCE_STAGE3 = 640, 10, 3   # Wait 3 epochs for improvement\n\nNGPUS = max(1, torch.cuda.device_count())\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPER_GPU_BATCH = 4\nACCUM_STEPS = 4\nLR = 5e-5\nWEIGHT_DECAY = 0.05\nWARMUP_EPOCHS = 5\nNUM_WORKERS = 2\nSEED = 42\n\nUSE_AMP = True\nUSE_GRAD_CHECKPOINT = True\nUSE_EMA = True\nEMA_DECAY = 0.9998\nUSE_SWA = True\nSWA_START = int(EPOCHS_STAGE1 * 0.8)\nUSE_ARCFACE = True\nARC_S, ARC_M = 30.0, 0.35\n\nMIXUP_ALPHA, CUTMIX_ALPHA, MIXUP_PROB, LABEL_SMOOTH = 0.8, 1.0, 1.0, 0.1\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:52:37.562543Z","iopub.execute_input":"2025-09-29T08:52:37.563147Z","iopub.status.idle":"2025-09-29T08:52:52.367178Z","shell.execute_reply.started":"2025-09-29T08:52:37.563116Z","shell.execute_reply":"2025-09-29T08:52:52.366638Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Cell 3: Models and Helper Modules**","metadata":{}},{"cell_type":"code","source":"def build_transforms(img_size):\n    train_tf = transforms.Compose([\n        transforms.RandomResizedCrop(img_size, scale=(0.5, 1.0)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ColorJitter(0.35,0.35,0.25,0.05),\n        transforms.RandAugment(num_ops=2, magnitude=9),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n        transforms.RandomErasing(p=0.25, scale=(0.02,0.33), ratio=(0.3,3.3))\n    ])\n    test_tf = transforms.Compose([\n        transforms.Resize(int(img_size * 1.15)),\n        transforms.CenterCrop(img_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n    ])\n    return train_tf, test_tf\n\nclass PatchAttention(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.qkv = nn.Conv2d(in_channels, in_channels * 3, kernel_size=1, bias=False)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n    def forward(self, x):\n        b, C, w, h = x.size()\n        q, k, v = self.qkv(x).view(b, 3, C, w * h).permute(1, 0, 2, 3).unbind(0)\n        q = q.permute(0, 2, 1)\n        att = self.softmax(torch.bmm(q, k))\n        out = torch.bmm(v, att.permute(0, 2, 1)).view(b, C, w, h)\n        return self.gamma * out + x\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30.0, m=0.5):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n        self.s, self.m = s, m\n        self.cos_m, self.sin_m = math.cos(m), math.sin(m)\n        self.th, self.mm = math.cos(math.pi - m), math.sin(math.pi - m) * m\n    def forward(self, input, label):\n        cosine = nn.functional.linear(nn.functional.normalize(input), nn.functional.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.clamp(cosine**2, 0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = torch.zeros_like(cosine).scatter_(1, label.view(-1, 1).long(), 1)\n        return self.s * (one_hot * phi + (1.0 - one_hot) * cosine)\n\nclass SYDNet(nn.Module):\n    def __init__(self, backbone_name, n_classes, drop_path_rate=0.2):\n        super().__init__()\n        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, drop_path_rate=drop_path_rate)\n        self.feat_dim = self.backbone.num_features\n        self.attention = PatchAttention(self.feat_dim)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(self.feat_dim, n_classes)\n        self.arcface = None\n    def forward(self, x, labels=None):\n        feats = self.backbone.forward_features(x)\n        if feats.dim() == 4:\n            feats = self.attention(feats)\n            feats = self.pool(feats).flatten(1)\n        if self.arcface is not None and labels is not None:\n            return self.arcface(feats, labels)\n        return self.classifier(feats)\n\ndef load_checkpoint_module(module, path):\n    if os.path.exists(path):\n        sd = torch.load(path, map_location='cpu')['state_dict']\n        module.load_state_dict(sd, strict=False)\n        return True\n    return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:53:03.459782Z","iopub.execute_input":"2025-09-29T08:53:03.460024Z","iopub.status.idle":"2025-09-29T08:53:03.473194Z","shell.execute_reply.started":"2025-09-29T08:53:03.460006Z","shell.execute_reply":"2025-09-29T08:53:03.472539Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Cell 4: The Resumable Training Pipeline**","metadata":{}},{"cell_type":"code","source":"import torch.amp\n\ndef train_stage(backbone, stage_name, img_size, epochs, patience, prev_ckpt=None, finetune_arc=False, freeze_backbone=False):\n    print(f\"\\n==== TRAIN {backbone} | {stage_name} | img {img_size} | epochs {epochs} (Patience: {patience}) ====\")\n    \n    stage_checkpoint_path = os.path.join(SAVE_DIR, f\"{backbone.replace('.','_')}_{stage_name}_checkpoint.pth\")\n    stage_best_model_path = os.path.join(SAVE_DIR, f\"{backbone.replace('.','_')}_{stage_name}_best.pth\")\n    \n    train_tf, val_tf = build_transforms(img_size)\n    train_ds = datasets.ImageFolder(os.path.join(DATA_PATH,\"train\"), transform=train_tf)\n    val_ds = datasets.ImageFolder(os.path.join(DATA_PATH,\"test\"), transform=val_tf)\n    n_classes = len(train_ds.classes)\n\n    train_loader = DataLoader(train_ds, batch_size=PER_GPU_BATCH * NGPUS, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=PER_GPU_BATCH * NGPUS, shuffle=False, num_workers=NUM_WORKERS)\n\n    model = SYDNet(backbone, n_classes)\n    if USE_GRAD_CHECKPOINT and hasattr(model.backbone, \"set_grad_checkpointing\"):\n        model.backbone.set_grad_checkpointing(True)\n\n    base_module = model\n    if NGPUS > 1: model = nn.DataParallel(model)\n    model.to(DEVICE)\n    \n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[\n        optim.lr_scheduler.LinearLR(optimizer, 1e-6, 1.0, total_iters=min(WARMUP_EPOCHS, epochs)),\n        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, epochs - WARMUP_EPOCHS), eta_min=1e-6)\n    ], milestones=[min(WARMUP_EPOCHS, epochs)])\n\n    scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n    \n    criterion = SoftTargetCrossEntropy() if stage_name.startswith(\"stage1\") else nn.CrossEntropyLoss()\n    mixup_fn = Mixup(mixup_alpha=MIXUP_ALPHA, cutmix_alpha=CUTMIX_ALPHA, prob=MIXUP_PROB, label_smoothing=LABEL_SMOOTH, num_classes=n_classes) if stage_name.startswith(\"stage1\") else None\n    \n    ema = ModelEmaV2(base_module, decay=EMA_DECAY) if USE_EMA else None\n    swa_model = AveragedModel(base_module) if USE_SWA else None\n    \n    start_epoch = 0\n    best_val = 0.0\n    patience_counter = 0\n    \n    found_resume_file = False\n    for root, _, files in os.walk(\"/kaggle/input/\"):\n        if os.path.basename(stage_checkpoint_path) in files:\n            stage_checkpoint_path = os.path.join(root, os.path.basename(stage_checkpoint_path))\n            found_resume_file = True\n            break\n\n    if os.path.exists(stage_checkpoint_path) and found_resume_file:\n        print(f\"Resuming from checkpoint: {stage_checkpoint_path}\")\n        checkpoint = torch.load(stage_checkpoint_path)\n        base_module.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        if ema and 'ema_state_dict' in checkpoint: ema.load_state_dict(checkpoint['ema_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_val = checkpoint['best_val_acc']\n        print(f\"Resumed from epoch {start_epoch}. Best accuracy so far: {best_val:.4f}\")\n    elif prev_ckpt and os.path.exists(prev_ckpt):\n        print(f\"Initializing from previous stage's best model: {os.path.basename(prev_ckpt)}\")\n        load_checkpoint_module(base_module, prev_ckpt)\n        \n    stage_checkpoint_path = os.path.join(SAVE_DIR, os.path.basename(stage_checkpoint_path))\n\n    if finetune_arc: model.module.arcface = ArcMarginProduct(model.module.feat_dim, n_classes, s=ARC_S, m=ARC_M).to(DEVICE)\n    if freeze_backbone:\n        for n,p in model.named_parameters(): p.requires_grad = any(h in n for h in [\"classifier\", \"arcface\", \"attention\"])\n\n    for epoch in range(start_epoch, epochs):\n        model.train()\n        for step, (imgs, labels) in enumerate(tqdm(train_loader, desc=f\"E{epoch+1}\")):\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            \n            if mixup_fn: imgs, labels = mixup_fn(imgs, labels)\n\n            with torch.amp.autocast(device_type='cuda', enabled=USE_AMP):\n                outs = model(imgs, labels) if finetune_arc else model(imgs)\n                loss = criterion(outs, labels) / ACCUM_STEPS\n            scaler.scale(loss).backward()\n            if (step + 1) % ACCUM_STEPS == 0:\n                scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n                if ema: ema.update(base_module)\n        \n        scheduler.step()\n        if swa_model and epoch >= SWA_START: swa_model.update_parameters(base_module)\n\n        eval_module = swa_model if swa_model and epoch >= SWA_START else (ema.module if ema else base_module)\n        eval_module.eval()\n        \n        correct = 0\n        with torch.no_grad():\n            for imgs_val, labels_val in val_loader:\n                imgs_val, labels_val = imgs_val.to(DEVICE), labels_val.to(DEVICE)\n                outputs = eval_module(imgs_val)\n                preds = torch.argmax(outputs, 1)\n                correct += (preds == labels_val).sum().item()\n        \n        val_acc = correct / len(val_ds)\n        print(f\"[{backbone} {stage_name}] Epoch {epoch+1} Val Acc: {val_acc:.4f}\")\n\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': base_module.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'ema_state_dict': ema.state_dict() if ema else None,\n            'best_val_acc': best_val,\n        }, stage_checkpoint_path)\n\n        if val_acc > best_val:\n            best_val = val_acc\n            patience_counter = 0\n            torch.save({'state_dict': eval_module.state_dict()}, stage_best_model_path)\n            print(f\"Saved best model checkpoint -> {stage_best_model_path} (acc={val_acc:.4f})\")\n        else:\n            patience_counter += 1\n            print(f\"No improvement in validation accuracy for {patience_counter} epoch(s). Patience is {patience}.\")\n            if patience_counter >= patience:\n                print(f\"\\nEarly stopping triggered after {patience} epochs with no improvement.\\n\")\n                break\n\n    final_best_path = stage_best_model_path\n    if swa_model:\n        update_bn(train_loader, swa_model, device=DEVICE)\n        swa_path = final_best_path.replace(\".pth\", \"_swa.pth\")\n        torch.save({'state_dict': swa_model.state_dict()}, swa_path)\n        return swa_path\n    return final_best_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:53:09.130125Z","iopub.execute_input":"2025-09-29T08:53:09.130861Z","iopub.status.idle":"2025-09-29T08:53:09.149237Z","shell.execute_reply.started":"2025-09-29T08:53:09.130834Z","shell.execute_reply":"2025-09-29T08:53:09.148545Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Cell 5: Main Orchestration**","metadata":{}},{"cell_type":"code","source":"\"\"\"\ndef main_training():\n    final_ckpts = {}\n    for backbone in TRAIN_BACKBONES:\n        # Pass the patience value for each stage to the training function\n        stage1_final_best_ckpt = train_stage(backbone, \"stage1\", STAGE1_IMG, EPOCHS_STAGE1, PATIENCE_STAGE1)\n        stage2_final_best_ckpt = train_stage(backbone, \"stage2\", STAGE2_IMG, EPOCHS_STAGE2, PATIENCE_STAGE2, prev_ckpt=stage1_final_best_ckpt)\n        stage3_final_best_ckpt = train_stage(backbone, \"stage3\", STAGE3_IMG, EPOCHS_STAGE3, PATIENCE_STAGE3, prev_ckpt=stage2_final_best_ckpt, finetune_arc=USE_ARCFACE, freeze_backbone=True)\n        final_ckpts[backbone] = stage3_final_best_ckpt\n    \n    print(\"\\n===== ALL TRAINING STAGES COMPLETE =====\")\n    print(\"Final saved model checkpoints:\")\n    for backbone, path in final_ckpts.items():\n        print(f\"- {backbone}: {path}\")\n\nif __name__ == \"__main__\":\n    main_training()\n    \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:53:15.616424Z","iopub.execute_input":"2025-09-29T08:53:15.616709Z","iopub.status.idle":"2025-09-29T16:52:57.691499Z","shell.execute_reply.started":"2025-09-29T08:53:15.616689Z","shell.execute_reply":"2025-09-29T16:52:57.685139Z"}},"outputs":[{"name":"stdout","text":"\n==== TRAIN convnext_large.fb_in22k_ft_in1k | stage1 | img 384 | epochs 120 (Patience: 15) ====\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/791M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a08d9af268e24e40a7116cdf4202c852"}},"metadata":{}},{"name":"stdout","text":"Resuming from checkpoint: /kaggle/input/end-of-session-1-1/sydnet_full_stages/convnext_large_fb_in22k_ft_in1k_stage1_checkpoint.pth\nResumed from epoch 16. Best accuracy so far: 0.9625\n","output_type":"stream"},{"name":"stderr","text":"E17: 100%|██████████| 1160/1160 [37:24<00:00,  1.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 17 Val Acc: 0.9741\nSaved best model checkpoint -> /kaggle/working/sydnet_full_stages/convnext_large_fb_in22k_ft_in1k_stage1_best.pth (acc=0.9741)\n","output_type":"stream"},{"name":"stderr","text":"E18: 100%|██████████| 1160/1160 [36:55<00:00,  1.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 18 Val Acc: 0.9762\nSaved best model checkpoint -> /kaggle/working/sydnet_full_stages/convnext_large_fb_in22k_ft_in1k_stage1_best.pth (acc=0.9762)\n","output_type":"stream"},{"name":"stderr","text":"E19: 100%|██████████| 1160/1160 [36:48<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 19 Val Acc: 0.9803\nSaved best model checkpoint -> /kaggle/working/sydnet_full_stages/convnext_large_fb_in22k_ft_in1k_stage1_best.pth (acc=0.9803)\n","output_type":"stream"},{"name":"stderr","text":"E20: 100%|██████████| 1160/1160 [36:47<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 20 Val Acc: 0.9803\nNo improvement in validation accuracy for 1 epoch(s). Patience is 15.\n","output_type":"stream"},{"name":"stderr","text":"E21: 100%|██████████| 1160/1160 [36:46<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 21 Val Acc: 0.9817\nSaved best model checkpoint -> /kaggle/working/sydnet_full_stages/convnext_large_fb_in22k_ft_in1k_stage1_best.pth (acc=0.9817)\n","output_type":"stream"},{"name":"stderr","text":"E22: 100%|██████████| 1160/1160 [36:50<00:00,  1.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 22 Val Acc: 0.9817\nNo improvement in validation accuracy for 1 epoch(s). Patience is 15.\n","output_type":"stream"},{"name":"stderr","text":"E23: 100%|██████████| 1160/1160 [36:52<00:00,  1.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 23 Val Acc: 0.9822\nSaved best model checkpoint -> /kaggle/working/sydnet_full_stages/convnext_large_fb_in22k_ft_in1k_stage1_best.pth (acc=0.9822)\n","output_type":"stream"},{"name":"stderr","text":"E24: 100%|██████████| 1160/1160 [36:21<00:00,  1.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 24 Val Acc: 0.0127\nNo improvement in validation accuracy for 1 epoch(s). Patience is 15.\n","output_type":"stream"},{"name":"stderr","text":"E25: 100%|██████████| 1160/1160 [36:08<00:00,  1.87s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 25 Val Acc: 0.0127\nNo improvement in validation accuracy for 2 epoch(s). Patience is 15.\n","output_type":"stream"},{"name":"stderr","text":"E26: 100%|██████████| 1160/1160 [36:08<00:00,  1.87s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 26 Val Acc: 0.0127\nNo improvement in validation accuracy for 3 epoch(s). Patience is 15.\n","output_type":"stream"},{"name":"stderr","text":"E27: 100%|██████████| 1160/1160 [36:08<00:00,  1.87s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage1] Epoch 27 Val Acc: 0.0127\nNo improvement in validation accuracy for 4 epoch(s). Patience is 15.\n","output_type":"stream"},{"name":"stderr","text":"E28:  72%|███████▏  | 834/1160 [26:00<10:09,  1.87s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1471088044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# This is the \"Go\" button for the long training run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmain_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/1471088044.py\u001b[0m in \u001b[0;36mmain_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbackbone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTRAIN_BACKBONES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Pass the patience value for each stage to the training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mstage1_final_best_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stage1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTAGE1_IMG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_STAGE1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE_STAGE1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mstage2_final_best_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stage2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTAGE2_IMG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_STAGE2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE_STAGE2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_ckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage1_final_best_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstage3_final_best_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stage3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTAGE3_IMG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_STAGE3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE_STAGE3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_ckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage2_final_best_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinetune_arc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_ARCFACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3003787590.py\u001b[0m in \u001b[0;36mtrain_stage\u001b[0;34m(backbone, stage_name, img_size, epochs, patience, prev_ckpt, finetune_arc, freeze_backbone)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# --- IMPROVEMENT 2: Updated autocast call ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_AMP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfinetune_arc\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mACCUM_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# This cell manually finishes the remaining stages for the Week 1 model.\n# Run this cell AFTER you have manually stopped the long run in Cell 5.\n\n# --- Manual Override (Corrected Version) ---\n\n# FIX: We explicitly define the backbone model for Week 1, since stopping\n# the previous cell removed it from memory.\nbackbone = \"convnext_large.fb_in22k_ft_in1k\"\n\n# We will use the best model saved from Stage 1, which is safe from the accuracy crash.\nstage1_final_best_ckpt = os.path.join(SAVE_DIR, f\"{backbone.replace('.','_')}_stage1_best.pth\")\n\nif os.path.exists(stage1_final_best_ckpt):\n    print(f\"✅ Found best model from Stage 1 with 98.22% accuracy.\")\n    print(f\"Force-starting Stage 2 using checkpoint: {os.path.basename(stage1_final_best_ckpt)}\")\n\n    # --- Run Stage 2 ---\n    # This will train on 512px images, starting from your best 384px model\n    stage2_final_best_ckpt = train_stage(\n        backbone, \"stage2\", STAGE2_IMG, EPOCHS_STAGE2, PATIENCE_STAGE2,\n        prev_ckpt=stage1_final_best_ckpt\n    )\n\n    # --- Run Stage 3 ---\n    # This will train on 640px images, starting from your best 512px model\n    stage3_final_best_ckpt = train_stage(\n        backbone, \"stage3\", STAGE3_IMG, EPOCHS_STAGE3, PATIENCE_STAGE3,\n        prev_ckpt=stage2_final_best_ckpt,\n        finetune_arc=USE_ARCFACE,\n        freeze_backbone=True\n    )\n\n    print(\"\\n\\n✅✅✅===== WEEK 1 TRAINING MANUALLY COMPLETED =====✅✅✅\")\n    print(f\"Final saved model for {backbone}:\")\n    print(stage3_final_best_ckpt)\n\nelse:\n    print(\"❌ ERROR: Could not find the best checkpoint file from Stage 1.\")\n    print(\"Please ensure the previous session was saved correctly.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:53:57.819701Z","iopub.execute_input":"2025-09-29T16:53:57.819974Z","iopub.status.idle":"2025-09-29T18:13:15.543508Z","shell.execute_reply.started":"2025-09-29T16:53:57.819953Z","shell.execute_reply":"2025-09-29T18:13:15.541415Z"}},"outputs":[{"name":"stdout","text":"✅ Found best model from Stage 1 with 98.22% accuracy.\nForce-starting Stage 2 using checkpoint: convnext_large_fb_in22k_ft_in1k_stage1_best.pth\n\n==== TRAIN convnext_large.fb_in22k_ft_in1k | stage2 | img 512 | epochs 40 (Patience: 8) ====\nInitializing from previous stage's best model: convnext_large_fb_in22k_ft_in1k_stage1_best.pth\n","output_type":"stream"},{"name":"stderr","text":"E1: 100%|██████████| 1160/1160 [54:27<00:00,  2.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"[convnext_large.fb_in22k_ft_in1k stage2] Epoch 1 Val Acc: 0.0069\nSaved best model checkpoint -> /kaggle/working/sydnet_full_stages/convnext_large_fb_in22k_ft_in1k_stage2_best.pth (acc=0.0069)\n","output_type":"stream"},{"name":"stderr","text":"E2:  29%|██▊       | 331/1160 [15:29<38:46,  2.81s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3857323050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# --- Run Stage 2 ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# This will train on 512px images, starting from your best 384px model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     stage2_final_best_ckpt = train_stage(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stage2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTAGE2_IMG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_STAGE2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE_STAGE2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprev_ckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstage1_final_best_ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3003787590.py\u001b[0m in \u001b[0;36mtrain_stage\u001b[0;34m(backbone, stage_name, img_size, epochs, patience, prev_ckpt, finetune_arc, freeze_backbone)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# --- IMPROVEMENT 2: Updated autocast call ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_AMP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfinetune_arc\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mACCUM_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"**Cell 6: Final Evaluation**","metadata":{}},{"cell_type":"code","source":"def final_evaluation():\n    print(\"\\n===== STARTING FINAL EVALUATION WITH ENSEMBLE + TTA =====\")\n    \n    # This list will hold the paths to the final, best model for each backbone\n    final_model_info = []\n    for backbone in TRAIN_BACKBONES:\n        # Find the final checkpoint file for each backbone (we'll look for the _swa.pth file)\n        ckpt_path = os.path.join(SAVE_DIR, f\"{backbone.replace('.','_')}_stage3_best_swa.pth\")\n        if not os.path.exists(ckpt_path):\n            # Fallback to the non-SWA model if SWA wasn't used or saved differently\n            ckpt_path = os.path.join(SAVE_DIR, f\"{backbone.replace('.','_')}_stage3_best.pth\")\n\n        if os.path.exists(ckpt_path):\n            final_model_info.append({'name': backbone, 'path': ckpt_path})\n        else:\n            print(f\"WARNING: Could not find final checkpoint for {backbone}. It will be excluded from the ensemble.\")\n\n    if not final_model_info:\n        print(\"ERROR: No trained models found for evaluation. Please run the training cell first.\")\n        return\n\n    # Load the models for the ensemble\n    models = []\n    # Get num_classes from the test dataset\n    num_classes = len(datasets.ImageFolder(os.path.join(DATA_PATH,'test')).classes)\n    \n    for info in final_model_info:\n        model = SYDNet(info['name'], num_classes, drop_path_rate=0.0) # No dropout for inference\n        load_checkpoint_module(model, info['path'])\n        if NGPUS > 1:\n            model = nn.DataParallel(model)\n        model.to(DEVICE).eval()\n        models.append(model)\n    \n    # Define TTA transforms\n    tta_tf = transforms.Compose([\n        transforms.Resize(int(STAGE3_IMG * 1.15)), transforms.TenCrop(STAGE3_IMG),\n        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(c) for c in crops])),\n        transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])(t) for t in tensors]))\n    ])\n    tta_ds = datasets.ImageFolder(os.path.join(DATA_PATH,\"test\"), transform=tta_tf)\n    tta_loader = DataLoader(tta_ds, batch_size=PER_GPU_BATCH, shuffle=False, num_workers=NUM_WORKERS)\n    \n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for imgs, labels in tqdm(tta_loader, desc=\"TTA Eval\"):\n            bs, ncrops, c, h, w = imgs.size()\n            imgs = imgs.view(-1, c, h, w).to(DEVICE)\n            \n            # Get predictions from all models in the ensemble\n            model_preds = sum(model(imgs).view(bs, ncrops, -1).mean(1).softmax(1) for model in models)\n            \n            all_preds.append(model_preds.cpu().numpy())\n            all_labels.append(labels.numpy())\n    \n    # Calculate final accuracy\n    all_preds_np = np.vstack(all_preds)\n    all_labels_np = np.concatenate(all_labels)\n    final_predictions = np.argmax(all_preds_np, axis=1)\n    accuracy = np.mean(final_predictions == all_labels_np)\n    \n    print(f\"\\nFINAL ENSEMBLED ACCURACY WITH TTA: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n\n# Run the final evaluation\nif __name__ == \"__main__\":\n    final_evaluation()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}